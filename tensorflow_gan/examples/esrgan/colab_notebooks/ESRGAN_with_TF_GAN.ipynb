{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESRGAN_with_TF_GAN (5).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH5RikVCeczA"
      },
      "source": [
        "# **ESRGAN with TF-GAN**\n",
        "\n",
        "### **Overview**\n",
        "This notebook demonstrates the E2E process of data loading, preprocessing, training and evaluation of the [ESRGAN](https://arxiv.org/abs/1809.00219) model using Tensorflow and Tf-GAN. To understand the basics of Tf-GAN and explore more features of the library, please visit [Tf-GAN tutorial](https://github.com/tensorflow/gan/blob/master/tensorflow_gan/examples/colab_notebooks/tfgan_tutorial.ipynb) notebook first. \n",
        "\n",
        "### **Steps to run this notebook**\n",
        "\n",
        "\n",
        "* Click on the icon to open this notebook in Google Colaboratory. \n",
        "\n",
        "> [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tensorflow/gan/tree/master/tensorflow_gan/examples/esrgan/colab_notebooks/ESRGAN_with_TF-GAN.ipynb)\n",
        "\n",
        "*  Navigate to `Runtime > Change runtime type` tab \n",
        "* Select GPU from hardware accelerator and save\n",
        "* Click Connect in the upper right corner and select Connect to hosted runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbPmnitLY7ab"
      },
      "source": [
        "# Check that imports for the rest of the file work.\n",
        "import os\n",
        "import tensorflow as tf\n",
        "!pip install tensorflow-gan\n",
        "import tensorflow_gan as tfgan\n",
        "import numpy as np\n",
        "import PIL \n",
        "from tensorflow.python.data.experimental import AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV9-ddGuZl6A"
      },
      "source": [
        "### **Learning Objectives**\n",
        "Through this Colab notebook you will learn how to :\n",
        "* Implement the ESRGAN model and train it\n",
        "* Make use of various Tf-GAN functions to visualize and evaluate the results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOxTwUEp43xD"
      },
      "source": [
        "### **Training ESRGAN**\n",
        "The ESRGAN model proposed in the paper [ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks (Wang Xintao et al.)](https://arxiv.org/abs/1809.00219) performs the task of image super-resolution which is the process of reconstructing high resolution (HR) image from a given low resolution (LR) image. Please go through the paper to have a deeper understanding of the model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj0cUphO2b8I"
      },
      "source": [
        "## Define Parameters\n",
        "\n",
        "Add paths to save the models and dataset. The batch size and dimension of HR Images needs to be reduced while training on Colab GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1GsSljo2bK8"
      },
      "source": [
        "Params = {\n",
        "  'batch_size' : 8,                   # Number of image samples used in each training step          \n",
        "  'hr_dimension' : 128,               # Dimension of a High Resolution (HR) Image\n",
        "  'scale' : 4,                        # Factor by which Low Resolution (LR) Images will be downscaled.\n",
        "  'data_name': 'div2k/bicubic_x4',    # Dataset name - loaded using tfds.\n",
        "  'trunk_size' : 11,                  # Number of Residual blocks used in Generator,\n",
        "  'init_lr' : 0.00005,                # Initial Learning rate for networks. \n",
        "  'ph1_steps' : 10000,                # Number of steps required for phase-1 training\n",
        "  'ph2_steps' : 100000,               # Number of steps required for phase-2 training\n",
        "  'decay_ph1' : 0.2,                  # Factor by which learning rates are modified during phase-1 training \n",
        "  'decay_ph2' : 0.5,                  # Factor by which learning rates are modified during phase-2 training \n",
        "  'model_dir' : '/content/',          # Path to save the model after training. (inside the cloud bucket)\n",
        "  'ckpt_dir' : '/content/ckpts/',     # Path to save the training checkpoints. (outside the cloud bucket)\n",
        "  'lambda' : 0.005,                   # To balance adversarial loss during phase-2 training. \n",
        "  'eta' : 0.01,                       # To balance L1 loss during phase-2 training.\n",
        "  'val_steps' : 100                   # Number of steps required for validation.\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFlwIkdgY_Gq"
      },
      "source": [
        "## Load Training Dataset\n",
        "We have used the [DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/) dataset which is usually used for benchmarking super resolution models. DIV2K dataset provides various kinds of image from which we are downloading only the HR images and corresponding LR images downsampled using bicubic downsampling. All the HR images are also scaled to 96 x 96 and LR images to 28 x 28.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taZ6mKX0KMVT"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def input_fn(mode, params):\n",
        "  assert 'batch_size' in params\n",
        "  bs = params['batch_size']\n",
        "  split = 'train' if mode == 'train' else 'validation'\n",
        "  shuffle = True \n",
        "\n",
        "  def scale(image, *args):\n",
        "    hr_size = params['hr_dimension']\n",
        "    scale = params['scale']\n",
        "\n",
        "    hr_image = image\n",
        "    hr_image = tf.image.resize(hr_image, [hr_size, hr_size])\n",
        "    lr_image = tf.image.resize(hr_image, [hr_size//scale, hr_size//scale], method='bicubic')\n",
        "    \n",
        "    hr_image = tf.clip_by_value(hr_image, 0, 255)\n",
        "    lr_image = tf.clip_by_value(lr_image, 0, 255)\n",
        "    \n",
        "    return lr_image, hr_image\n",
        "\n",
        "  dataset = (tfds.load(params['data_name'], split=split, as_supervised=True)\n",
        "             .map(scale, num_parallel_calls=4)\n",
        "             .cache()\n",
        "             .repeat())\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(\n",
        "        buffer_size=10000, reshuffle_each_iteration=True)\n",
        "  dataset = (dataset.batch(bs, drop_remainder=True)\n",
        "               .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  \n",
        "  return iter(dataset)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klj29dH5Yx1r"
      },
      "source": [
        "train_ds = input_fn(mode='train', params=Params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fEXZCm4T3W7"
      },
      "source": [
        "## Visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu9XD_myIkb4"
      },
      "source": [
        "img_lr, img_hr = next(iter(train_ds))\n",
        "lr = PIL.Image.fromarray(np.array(img_lr)[0].astype(np.uint8))\n",
        "lr = lr.resize([256, 256])\n",
        "display(lr)\n",
        "\n",
        "hr = PIL.Image.fromarray(np.array(img_hr)[0].astype(np.uint8))\n",
        "hr = hr.resize([256, 256])\n",
        "display(hr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN12V5iSZBdv"
      },
      "source": [
        "## Network Architecture\n",
        "The basic network buidling unit of the  ESRGAN is the Residual-in-Residual Block (RRDB) without batch normalization. The network implemented is similar to the architecture proposed in the paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naxHBKvedtfz"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, Concatenate, Lambda, Flatten, Dense\n",
        "from keras.layers import LeakyReLU, Conv2D, BatchNormalization, Conv2DTranspose"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrMl2UMxgxt5"
      },
      "source": [
        "\n",
        "### Generator Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKmtmvCbb8yt"
      },
      "source": [
        "def _conv_block(input, filters, activation=True):\n",
        "    h = Conv2D(filters, kernel_size=[3,3], \n",
        "               kernel_initializer=\"he_normal\", bias_initializer=\"zeros\", \n",
        "               strides=[1,1], padding='same', use_bias=True)(input)\n",
        "    if activation:\n",
        "        h = LeakyReLU(0.2)(h)\n",
        "    return h\n",
        "\n",
        "def dense_block(input):\n",
        "    h1 = _conv_block(input, 32)\n",
        "    h1 = Concatenate()([input, h1])\n",
        "\n",
        "    h2 = _conv_block(h1, 32)\n",
        "    h2 = Concatenate()([input, h1, h2])\n",
        "\n",
        "    h3 = _conv_block(h2, 32)\n",
        "    h3 = Concatenate()([input, h1, h2, h3])\n",
        "\n",
        "    h4 = _conv_block(h3, 32)\n",
        "    h4 = Concatenate()([input, h1, h2, h3, h4])  \n",
        "\n",
        "    h5 = _conv_block(h4, 32, activation=False)\n",
        "    \n",
        "    h5 = Lambda(lambda x: x * 0.2)(h5)\n",
        "    h = Add()([h5, input])\n",
        "    \n",
        "    return h\n",
        "\n",
        "def RRDB(input):\n",
        "    h = dense_block(input)\n",
        "    h = dense_block(h)\n",
        "    h = dense_block(h)\n",
        "    h = Lambda(lambda x:x * 0.2)(h)\n",
        "    out = Add()([h, input])\n",
        "    return out\n",
        "\n",
        "def upsample(x, filters):\n",
        "  x = Conv2DTranspose(filters, kernel_size=3, \n",
        "                      strides=2, padding='same', \n",
        "                      use_bias = True)(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  return x\n",
        "\n",
        "def ESRGAN_G(filter=32, trunk_size=Params['trunk_size'], out_channels=3):\n",
        "    lr_input = Input(shape=(None, None, 3))\n",
        "    \n",
        "    x = Conv2D(filter, kernel_size=[3,3], \n",
        "               strides=[1,1], padding='same', \n",
        "               use_bias=True)(lr_input)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "\n",
        "    ref = x\n",
        "    \n",
        "    for i in range(trunk_size):\n",
        "        x = RRDB(x)\n",
        "\n",
        "    x = Conv2D(filter, kernel_size=[3,3], strides=[1,1], padding='same', use_bias = True)(x)\n",
        "    x = Add()([x, ref])\n",
        "\n",
        "    x = upsample(x, filter)\n",
        "    x = upsample(x, filter)\n",
        "    \n",
        "    x = Conv2D(filter, kernel_size=3, strides=1, padding='same', use_bias=True)(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    hr_output = Conv2D(out_channels, kernel_size=3, strides=1, padding='same', use_bias=True)(x)\n",
        "\n",
        "    model = Model(inputs=lr_input, outputs=hr_output)\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LVuaWw9ZC6x"
      },
      "source": [
        "### Discriminator Network\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaV9XIkZdJYc"
      },
      "source": [
        "def _conv_block_d(x, out_channel):\n",
        "  x = Conv2D(out_channel, 3,1, padding='same', use_bias=False)(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  x = Conv2D(out_channel, 4,2, padding='same', use_bias=False)(x)\n",
        "  x = BatchNormalization(momentum=0.8)(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)\n",
        "  return x\n",
        "\n",
        "def ESRGAN_D(filters = 64, training=True):\n",
        "    img = Input(shape = (Params['hr_dimension'], Params['hr_dimension'], 3))\n",
        "    \n",
        "    x = Conv2D(filters, [3,3], 1, padding='same', use_bias=False)(img)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = Conv2D(filters, [3,3], 2, padding='same', use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = _conv_block_d(x, filters *2)\n",
        "    x = _conv_block_d(x, filters *4)\n",
        "    x = _conv_block_d(x, filters *8)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(100)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs = img, outputs = x)\n",
        "    return model"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj3OWMkLfDuD"
      },
      "source": [
        "## Loss Functions\n",
        "The ESRGAN model makes use of three loss functions - pixel loss, perceptual loss (vgg_loss) and adversarial loss. Perceptual loss is calculated using the pre-trained VGG-19 network. Adversarial loss is separately calculated for both generator (relativistic_avg_loss_g) and discriminator (relativistic_avg_loss_d) using relativistic discriminator discussed in the paper. \n",
        "\n",
        "These loss functions ensures the balance between visual quality and metrics such as PSNR and encorages the generator to produce more realistic images with natural textures.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAOcRRJ7fHWR"
      },
      "source": [
        "def pixel_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    return tf.reduce_mean(tf.reduce_mean(tf.abs(y_true - y_pred), axis = 0))\n",
        "\n",
        "# Functions for calculating adversarial loss. \n",
        "def ragan_discriminator_loss(d_real, d_fake):\n",
        "  def get_logits(x, y):\n",
        "    return x - tf.reduce_mean(y)\n",
        "  \n",
        "  real_logits = get_logits(d_real, d_fake)\n",
        "  fake_logits = get_logits(d_fake, d_real)\n",
        "\n",
        "  real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "          labels=tf.ones_like(real_logits), logits=real_logits))\n",
        "  fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "      labels=tf.zeros_like(fake_logits), logits=fake_logits))\n",
        "\n",
        "  return real_loss + fake_loss\n",
        "\n",
        "def ragan_generator_loss(d_real, d_fake):\n",
        "  real_logits = d_real - tf.reduce_mean(d_fake)\n",
        "  fake_logits = d_fake - tf.reduce_mean(d_real)\n",
        "  \n",
        "  real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "      labels=tf.zeros_like(real_logits), logits=real_logits))  \n",
        "  fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "      labels=tf.ones_like(fake_logits), logits=fake_logits))\n",
        "\n",
        "  return real_loss + fake_loss\n",
        "  \n",
        "# Function for calculating perceptual loss\n",
        "def vgg_loss(weight=None, input_shape=None):\n",
        "  vgg_model = tf.keras.applications.vgg19.VGG19(\n",
        "      input_shape=input_shape, weights=weight, include_top=False\n",
        "  )\n",
        "\n",
        "  for layer in vgg_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  vgg_model.get_layer(\"block5_conv4\").activation = lambda x: x\n",
        "  vgg = tf.keras.Model(\n",
        "      inputs=[vgg_model.input],\n",
        "      outputs=[vgg_model.get_layer(\"block5_conv4\").output])\n",
        "\n",
        "  def loss(y_true, y_pred):\n",
        "      return tf.compat.v1.losses.absolute_difference(vgg(y_true), vgg(y_pred))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2RGB6fGZFgx"
      },
      "source": [
        "## Training\n",
        "ESRGAN model is trained in two phases in which the first phase deals with training the generator network individually and is aimed at improving the PSNR values of generated images by reducing the L1 loss.  \n",
        "\n",
        "Training of the same generator model is continued in the second phase along with the discriminator network. In the second phase, the generator reduces the L1 Loss, Relativistic average GAN (RaGAN) loss which indicates how realistic does the generated image look and the imporved Perceptual loss proposed in the paper.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP8uNVSx96y4"
      },
      "source": [
        "# To display images in the order : LR Image -> Generated Image -> HR Image\n",
        "def visualize_results(image_lr, generated, image_hr):\n",
        "    size = 128\n",
        "    resized_lr = tf.image.resize(image_lr, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n",
        "    resized_gen = tf.image.resize(generated, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n",
        "    resized_hr = tf.image.resize(image_hr, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "    stack = tf.stack([resized_lr[0], resized_gen[0], resized_hr[0]])\n",
        "\n",
        "    image_grid = tfgan.eval.python_image_grid(stack, grid_shape=(1, 3))\n",
        "    result = PIL.Image.fromarray(image_grid.astype(np.uint8))\n",
        "    return result"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOZiTE4EfCcW"
      },
      "source": [
        "### Phase - 1 Training\n",
        "\n",
        "Steps Involved:\n",
        "\n",
        "* Define the generator and its optimizer. \n",
        "* Take LR, HR image pairs from the training dataset\n",
        "* Input the LR image to the generator network\n",
        "* Calculate the L1 loss using the generated image and HR image\n",
        "* Calculate gradient value and apply it to the optimizer\n",
        "* Update the learning rate of optimizer after every decay steps for better performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR-ubHlskrD1"
      },
      "source": [
        "metric = tf.keras.metrics.Mean()\n",
        "psnr_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "generator = ESRGAN_G()\n",
        "\n",
        "G_optimizer = tf.optimizers.Adam(\n",
        "    learning_rate = 0.0002,\n",
        "    beta_1 = 0.9,\n",
        "    beta_2 = 0.99\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tIjTGN2g_qY"
      },
      "source": [
        "def train_step(image_lr, image_hr):\n",
        "  with tf.GradientTape() as tape:\n",
        "    fake = generator(image_lr)\n",
        "    loss = pixel_loss(image_hr, fake) * (1.0 / Params['batch_size'])\n",
        "    psnr_value = tf.image.psnr(fake, image_hr,max_val = 256.0)\n",
        " \n",
        "    metric(loss)\n",
        "\n",
        "    gradient = tape.gradient(loss, generator.trainable_variables)\n",
        "    G_optimizer.apply_gradients(zip(gradient, generator.trainable_variables))\n",
        "\n",
        "    return psnr_value\n",
        "\n",
        "def val_steps(image_lr, image_hr):\n",
        "  fake = generator(image_lr)\n",
        "  result = visualize_results(image_lr, fake, image_hr)\n",
        "  display(result)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkJ_A8E8dnBY"
      },
      "source": [
        "step_count = 0\n",
        "\n",
        "while step_count < Params['ph1_steps']:\n",
        "  lr, hr = next(train_ds)\n",
        "  psnr_value = train_step(lr, hr)\n",
        "  psnr_metric(psnr_value)\n",
        "  \n",
        "  if step_count%1000 == 0:\n",
        "    print(\"step {}      PNSR = {}\".format(step_count, psnr_metric.result()))\n",
        "    val_steps(lr, hr) \n",
        "  \n",
        "  if step_count%5000 == 0:\n",
        "    G_optimizer.learning_rate.assign(\n",
        "        G_optimizer.learning_rate * Params['decay_ph1'])\n",
        "  step_count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOmQezFkn9w4"
      },
      "source": [
        "# Save the generator network which is then used for phase-2 training\n",
        "os.makedirs(Params['model_dir'] + '/Phase_1/generator', exist_ok = True)\n",
        "generator.save(Params['model_dir'] + '/Phase_1/generator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZy7ed72NyQh"
      },
      "source": [
        "\n",
        "###**Phase - 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXk5H75-gMlW"
      },
      "source": [
        "#### Define optimizers and load networks\n",
        " * Generator network trained in Phase 1 is loaded.\n",
        " * Checkpoints are also defined which can be useful during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRj1AM4XNxW5"
      },
      "source": [
        "optimizer = tf.optimizers.Adam(\n",
        "    learning_rate = 0.0002,\n",
        "    beta_1 = 0.9,\n",
        "    beta_2 = 0.99\n",
        ")\n",
        "\n",
        "generator = tf.keras.models.load_model(Params['model_dir'] + 'Phase_1/generator')\n",
        "discriminator = ESRGAN_D()\n",
        "\n",
        "G_optimizer = optimizer\n",
        "G_optimizer.learning_rate.assign(0.00005)\n",
        "D_optimizer = optimizer\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(G=generator,\n",
        "                                 D = discriminator,\n",
        "                                 G_optimizer=G_optimizer,\n",
        "                                 D_optimizer=D_optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvMN9RQPOB0a"
      },
      "source": [
        "perceptual_loss = vgg_loss(\n",
        "        weight = \"imagenet\",\n",
        "        input_shape = [Params['hr_dimension'], Params['hr_dimension'], 3])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEPMxS61OB28"
      },
      "source": [
        "gen_metric = tf.keras.metrics.Mean()\n",
        "disc_metric = tf.keras.metrics.Mean()\n",
        "psnr_metric = tf.keras.metrics.Mean()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxRZelMdOGOT"
      },
      "source": [
        "def train_step(image_lr, image_hr):\n",
        "   with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "     fake = generator(image_lr)\n",
        "\n",
        "     percep_loss = tf.reduce_mean(perceptual_loss(image_hr, fake)) \n",
        "     l1_loss = pixel_loss(image_hr, fake) \n",
        "     \n",
        "     real_logits = discriminator(image_hr) \n",
        "     fake_logits = discriminator(fake) \n",
        "     loss_RaG = ragan_generator_loss(real_logits, fake_logits) \n",
        "     disc_loss = ragan_discriminator_loss(real_logits, fake_logits) \n",
        "\n",
        "     gen_loss = percep_loss + Params['lambda'] * loss_RaG + Params['eta'] * l1_loss\n",
        "\n",
        "     gen_loss = gen_loss * (1.0 / Params['batch_size'])\n",
        "     disc_loss = disc_loss * (1.0 / Params['batch_size'])\n",
        "     psnr_loss = tf.image.psnr(fake, image_hr, max_val = 256.0)\n",
        "     \n",
        "     disc_metric(disc_loss) \n",
        "     gen_metric(gen_loss)\n",
        "     psnr_metric(psnr_loss)\n",
        "     \n",
        "     disc_grad = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "     D_optimizer.apply_gradients(zip(disc_grad, discriminator.trainable_variables))\n",
        "\n",
        "     gen_grad = gen_tape.gradient(gen_loss, generator.trainable_variables) \n",
        "     G_optimizer.apply_gradients(zip(gen_grad, generator.trainable_variables))\n",
        "     \n",
        "     return [disc_loss, gen_loss, psnr_loss]\n",
        "\n",
        "def val_steps(image_lr, image_hr):\n",
        "  fake = generator(image_lr)\n",
        "  result = visualize_results(image_lr, fake, image_hr)\n",
        "  display(result)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEKF1FLNOGQo"
      },
      "source": [
        "step_count = 0\n",
        "decay_step = [9000, 30000, 80000, 100000]\n",
        "\n",
        "while step_count < Params['ph2_steps']:  \n",
        "  lr, hr = next(train_ds)\n",
        "  \n",
        "  if tf.train.latest_checkpoint(Params['ckpt_dir']): \n",
        "    checkpoint.restore(tf.train.latest_checkpoint(Params['ckpt_dir']))\n",
        "\n",
        "  disc_loss, gen_loss, psnr_loss = train_step(lr, hr)\n",
        "\n",
        "  if step_count % 1000 == 0:\n",
        "    print(\"step {}\".format(step_count) + \"   Generator Loss = {}   \".format(gen_metric.result()) + \n",
        "          \"Disc Loss = {}\".format(disc_metric.result()) + \"   PSNR : {}\".format(psnr_metric.result()))\n",
        "    val_steps(lr, hr)\n",
        "  \n",
        "  checkpoint.write(Params['ckpt_dir'])\n",
        "  step_count += 1\n",
        "\n",
        "  if step_count >= decay_step[0]:\n",
        "    decay_step.pop(0)\n",
        "    G_optimizer.learning_rate.assign(\n",
        "              G_optimizer.learning_rate * Params['decay_ph2'])\n",
        "    \n",
        "    D_optimizer.learning_rate.assign(\n",
        "        D_optimizer.learning_rate * Params['decay_ph2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGLSP8c83XCj"
      },
      "source": [
        "generator.save(Params['model_dir'] + 'Phase_2/generator/')\n",
        "discriminator.save(Params['model_dir'] + 'Phase_2/discriminator/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRJTUeUlPyEP"
      },
      "source": [
        "###**Network Interpolation**\n",
        "Network Interpolation as explained in the paper is used to balance the effect of the generator trained during phase 1 and phase 2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xezj8tHRNLYo"
      },
      "source": [
        "def network_interpolation(alpha=0.2,\n",
        "                          phase_1_path=None,\n",
        "                          phase_2_path=None):\n",
        "  psnr_gen = tf.keras.models.load_model(phase_1_path)\n",
        "  gan_gen = tf.keras.models.load_model(phase_2_path)\n",
        "\n",
        "  for var_1, var_2 in zip(gan_gen.trainable_variables, \n",
        "                          psnr_gen.trainable_variables):\n",
        "    var_1.assign((1 - alpha) * var_2 + alpha * var_1)\n",
        "\n",
        "  return gan_gen"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb7M3s3AOCwC"
      },
      "source": [
        "generator = network_interpolation(phase_1_path = Params['model_dir'] + '/Phase_1/generator',\n",
        "                                  phase_2_path = Params['model_dir'] + '/Phase_2/generator')\n",
        "generator.save(Params['model_dir'] + '/InterpolatedGenerator/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXqL3xv3ZLAO"
      },
      "source": [
        "## Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ_qj9NoT3WM"
      },
      "source": [
        "val_ds = input_fn(mode='validation', params=Params)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC1BvUybPF8h"
      },
      "source": [
        "### Visualize Generated Images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv1ehR8APZLu"
      },
      "source": [
        "def val_steps(image_lr, image_hr):\n",
        "  fake = generator(image_lr)\n",
        "  result = visualize_results(image_lr, fake, image_hr)\n",
        "  display(result)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GEvV0mkOWpv"
      },
      "source": [
        "for i in range(3):\n",
        "  lr, hr = next(iter(val_ds))\n",
        "  val_steps(lr, hr) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsIpzvrJPKKl"
      },
      "source": [
        "### Calculate FID and Inception Scores\n",
        "\n",
        "FID and Inception Scores are two common metrices used to evaluate the performance of a GAN model and PSNR value is used to quantify the similarity between two images and is used for benchmarking super resolution models. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GawX1IyuTBB2"
      },
      "source": [
        "def get_fid_score(real_image, gen_image):\n",
        "  size = tfgan.eval.INCEPTION_DEFAULT_IMAGE_SIZE\n",
        "\n",
        "  resized_real_images = tf.image.resize(real_image, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n",
        "  resized_generated_images = tf.image.resize(gen_image, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n",
        "  \n",
        "  num_inception_images = 8\n",
        "  num_batches = Params['batch_size'] // num_inception_images\n",
        "  \n",
        "  fid = tfgan.eval.frechet_inception_distance(resized_real_images, resized_generated_images, num_batches=num_batches)\n",
        "  return fid\n",
        "\n",
        "def get_inception_score(images, num_inception_images = 8):\n",
        "  size = tfgan.eval.INCEPTION_DEFAULT_IMAGE_SIZE\n",
        "  resized_images = tf.image.resize(images, [size, size], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "  num_batches = Params['batch_size'] // num_inception_images\n",
        "  inc_score = tfgan.eval.inception_score(resized_images, num_batches=num_batches)\n",
        "\n",
        "  return inc_score"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA7P5JCUkjoi"
      },
      "source": [
        "generator = tf.keras.models.load_model(Params['model_dir'] + '/InterpolatedGenerator')\n",
        "\n",
        "fid_metric = tf.keras.metrics.Mean()\n",
        "inc_metric = tf.keras.metrics.Mean()\n",
        "psnr_metric = tf.keras.metrics.Mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyM6XciuzPWf"
      },
      "source": [
        "count = 0\n",
        "i = 0\n",
        "while i < Params['val_steps']: \n",
        "  lr, hr = next(val_ds)\n",
        "  \n",
        "  gen = generator(lr)\n",
        "  fid = get_fid_score(hr, gen)\n",
        "  real_is = get_inception_score(hr)\n",
        "  gen_is = get_inception_score(gen)\n",
        "\n",
        "  fid_metric(fid)\n",
        "  inc_metric(gen_is)\n",
        "  psnr_metric(tf.reduce_mean(tf.image.psnr(gen, hr, max_val = 256.0)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}